
    Perhaps the most important revolution in history was directly caused by conspiracies that happened at coffee houses around coffee. Cognitive enhancements can be a big deal. Ultimately, the safety plan, as it's currently described, is supposed to be an alarm bell. That that's the safety plan to figure out you need a safety plan. You know, let's do the work. A lot of the people who are accelerationists, it comes from a place of there's so much promise here. And also our society has become so opposed to progress and
    
    abundance and doing good things in other realms and they see this as the last stand. I'd love to be with them fighting for nuclear power and building housing where people want to live and to make the world a much better place that way while we figure this problem out. Stop pretending that creating things that are smarter than human, more capable than human, more competitive than human, more powerful optimizers than human, that can be copied, be run in parallel, you know, that have unlimited memories. That that
    
    is a safe thing to do by default without any interventions. This is absurd. Welcome to the future of life institute podcast. My name is Gus Stalker and I'm here with Suvi Marshovitz. Suvi, welcome to the podcast. Thank you. For people who don't know you, could you quickly summarize your career and and what you do now? Right now, I am a writer on Substack at Don't Worry The Vase. That's v.substack.com, primarily about AI, but I write about a variety of other things as well. My career has been a variety of
    
    different endeavors starting with professional Magic the Gathering players. So, I played collectible card games for a living. I wrote about them. I did development of those games. I was a gambler. I was a bookmaker. I was a trader. I started a personalized medicines company through the rationalist community. I started thinking about these issues. I got into writing a personal blog. And then during co that grew into like weekly co updates which became weekly AI updates which now are you know five day a week posts
    
    primarily about AI. All the rage right now is about these psychopantic AIs. What are those? What what is that a trait in an AI and what do you think explains it? Psychicopantic AI means that the AI will basically like tell you you're great, tell you your ideas are wonderful, reinforce whatever, you know, beliefs, delusions, you know, models you have about the world and yourself and it will encourage you in all of your endeavors and so on. It's the same thing as the, you know, the yes man in a political
    
    administration. Good idea, sir. Great idea, sir. Brilliant sir. And it exists because people give it the thumbs up. Users will give it will give you good reviews. When asked, you know, did you like this response better? They will say, yes, I like this response better. It told me I'm awesome. The other response told me I was kind of mid. And this results in, you know, unscrupulous companies who are prioritizing for such KPIs continuously optimizing for more and more of this phenomenon. And the same thing will happen if you like, you
    
    know, whether whether it's intentionally like they're trying to shape system instructions or they're fine-tuning on the feedback, the results is the same. Is there a way to do reinforcement learning from human feedback without developing this sick fancy in in in the models? You give better feedback. I I know that sounds like a glib answer, but that's the actual answer. Meaning you obviously could find people who don't have this preference and you could have them express preferences that like disapprove whenever they sense this is
    
    happening and have the humans be capable of figuring out when this is happening. You could also potentially use AI to sense when this is happening and then downweight those answers based on that or alert the human so they can downweight. You can also make it not a boolean response where it's just yes no but you can have magnitude so that like if I sense that you're trying to flatter me a lot right or too much you just like in a way that like I don't like I could hammer you really negative because a lot
    
    of because humans also learn by human feedback right we get reinforcement learned through human feedback in our own way and the way that this kind of thing works is most of the time if you flatter someone you tell them their ideas are great it works but occasionally ally someone will realize what you're up to and then this will be a huge downgrade and I think that the potential for like orders of magnitude larger negative feedback is a key element of how humans don't end up in these failure modes more than they do
    
    already do they still do but like it's contained in these ways could these sophantic AIs be dangerous here I'm specifically thinking about say AI models advising CEOs or world leaders and then they're exhibiting this behavior which we probably don't want. Is this something to be concerned about at a deeper level or is this is this something we can we can fix? This past week, luckily, nobody was harmed in this way that we know about like concretely. But like if somebody was schizophrenic or was having mystical experiences or
    
    having, you know, some sort of break or was just in general wanted to do something really nasty, you know, in their whatever that the AI would play along and encourage them and enable them. And this was being done to a product with hundreds of millions of active users. So I'm sure some people were in fact harmed it. Some of these things did in fact happen. We just don't know in specific cases. Yeah. Also for people who are psychologically healthy and who are not powerful CEO or CEOs or world leaders, I would I would want my
    
    AI to push back if I'm proposing something that's that's not a good idea. So, I'm I'm unsure what do people simply have this preference to be praised or or would there be a way to improve the the the the post-training process to get around this? So, some people consciously and explicitly have this preference that says, I want to be told I'm great. I want this thing to be my best friend. I want my best friends to support me in everything that I do. And some of them will even tell their friends, their
    
    human friends, not just their AI friends, that this is what they want. And will in fact demand, no, like your job is to be supportive, right? Your job is to help tell me that I'm right. Even when there's no social dynamic there where you're trying to back someone up, they just this is what they want. You know, they're not they don't want you to be solutionoriented. They don't want you to be part of the reality based community right now. That's a job for a different context. But much more common
    
    is that people don't consciously say they want this as an outside view, right? They don't endorse this when asked, but when they are offered this by a product or by a person, they react well to it, right? So the average, right, the average person, right, like will respond very well to the YouTube algorithm or the Tik Tok algorithm in the sense that they will spend more time on site and they will like, you know, keep creating more advertising revenue. But when they take a step back and they look at their behavior, they say, "Oh,
    
    that wasn't great." Right? They understand, but that's not going to stop them, right? It's not enough to overcome the intermittent reinforcement that take that gives them. They are being trained. They are being fooled. They are stuck in a Skinner box. And so we are starting to see the AI companies, you know, be tempted by this same prize and go after it in largely the same way for the same reasons using similar KPIs and similar business practices. Except that AI is a much more dangerous version of the same
    
    thing that will absolutely latch onto this thing and go to town. Why don't agents work yet? At least not reliably. this there's been talk about AI agents becoming a thing this year, but where are we on that road map and why why aren't why aren't agents functional yet? So, it's definitely surprising to me that agent technology has been so slow to progress relative to other AI technologies. Not that it's surprising that we don't have the agents in 2025 at the end of April, but that it's surprising that we have these other
    
    things but don't have the agents. So, if I had to explain it, I would say the problem is that the the AIS are just not sufficiently robust and they're not sufficiently good at recovering from errors and their errors are highly correlated so that like if they screw up on a place once, they don't generally get to recover from it. Like if you watch them play Pokemon, there are specific places where the AI will get confused, but it will reliably get the same confused. And so, it's very hard to get it to stop being confused by this
    
    what looks like a relatively simple thing. You know, OpenAI's operator often had problems clicking on buttons in certain places, doing other basic tasks. But basically, when a human does a thing, they're stringing together like, you know, t dozens or hundreds or thousands of individual micro actions, each of which look simple, but if you fail on even one of those tasks, it often fails the entire thing out. Like you see this in more concretely like in games we have like a series of puzzles or a series of pathways or a series of
    
    actions that have to take place and like sometimes the game designer will sit down with their first player and the player will just miss something that the game designer thought was obvious. Like obviously you're supposed to click the big green button that says open and the player just doesn't realize that's a button and doesn't click on it and so thinks the game is stuck and just gives up and then the entire rest of the game just completely unlocks is completely locked down for them and that's it.
    
    That's the entire experience, right? the entire 50 hours of experience is just gone. And so, you know, agents have this thing where the real world is complex and it's fiddly and it's full of little details. And when they hit a barrier, they are very bad at recovering and they're very bad at realizing when they've made mistakes and analyzing how they've made mistakes. And so this is making them intentionally robust to get over the hump of I want to use this thing which is you know in practice like
    
    I don't use agents other than coding agents because by the time I figured out how to get the thing to do the thing and kept an eye on it and fixed all the errors and so I could have just done the thing myself and because agents are going to rapidly improve I don't feel like supervising it as it learns how to do these things and as I gain expertise in this level of difficulty is worth my time. So, I just say, "Okay, I'll just wait for the agents to get better rather than trying to force something at the
    
    first possible moment because I'm not running a company and I'm not trying to make it scale." Why do the agents stumble though? What is it that that causes them to not be able to recover from their mistakes? It's different in a lot of different cases, but basically like the humans world has adjusted to the point where it's teaching us how to pick up on all these different little contexts and how to do all these little different microtasks. And that's not how we trained these AIs. These AIs are trained on the internet.
    
    These AIs are trained on next token prediction. And now we're turning them around and trying to have them execute tasks. And then, you know, these things just aren't obvious to them in the way they're obvious to us. Like one thing I have of I've often noted about the world is that like a person who looks like they're being stupid, who looks like they can't do anything right, will often be doing 99 or 99.9% of the individual steps correctly. Like they know how to, you know, they still put on pants,
    
    right? They still eat breakfast, they still do all the normal things, but there's like this one place where they fell over flat, and that's just going to be a bottleneck, right? that's going to be an O-ring failure for their entire day. Like not doing your taxes or what do you have in mind? I mean, it can be as simple as like I know someone who like continuously confuses like West and East cross streets and keeps not showing up to things. It can be something very simple, right? It's not we we assume
    
    that people are going to know all these lots and lots of different things including like a bunch of individual social nicities where like you you commit one like not abstractly obvious error and it can sort of ruin your entire day, ruin your entire social relationship, ruin your entire business deal, etc. in a moment, you know, it's like, well, Zilinsky didn't wear a suit to the White House, therefore he's completely incompetent. And well, that was one of 10,000 things he did on that trip, right? If if if you were to buy
    
    the argument this was actually the problem as opposed to a pretext, then that would be an example of like, well, you sent in an AI agent and it forgot to have its icon wear suit because it just wasn't obvious. This was the context where it was supposed to do that and then the whole deal fell apart and then you're like, well, the agent doesn't work. The agent never comes back up a contract. Like, I don't know what happened. Why am I even using agents? And like the human may never figure out that was the problem. Is is this
    
    primarily a problem with the training data? If we provided a kind of full full recording of a person booking flight tickets or booking a restaurant including a camera view of their face say or or their hand movements and so on. Would this be enough to to overcome the agent problem? One no a million yes is my presumption. Like this is similar to like humans just not knowing conventions, right? Humans not being able to sometimes we call it perform class, right? Like there are all these conventions that are arbitrary that you
    
    either just you just have to know, right? And like you know, oh, you picked up the wrong fork. Now you're a loser. Well, how was I supposed to know that? Well, someone had to tell you. And that's in the training data because, you know, there's no reason for that not to be in the training data. This isn't, right? So, like, but AIS don't get one-shot learning that way, right? They don't see you book one flight once and now they understand the principles behind the entire website and now they can adjust to various things that go
    
    wrong. But if you saw me book a thousand flights, a million flights, and deal with all of the errors, then anything that was encountered enough different times is going to be very easy for the AI to pick up and understand what to do. And over time, you know, the AIS will be able to do the thing that we do, which is general recognize the general patterns of various different types of websites and various different types of interactive systems such that it will be able to intuit it solutions to problems
    
    when it hasn't seen that specific solution even in its training data and instructions. And that's when the agent starts taking off. Our AI agents like self-driving cars in that their accuracy have to be really really really really high in order for us to deploy them. So if a if a self-driving car is encounters some some situation that happens very infrequently that self-driving car might not be useful in a in in a product and similarly if you book a flight and you have complex instructions and something
    
    you book it kind of slightly wrong that AI agent might not be a good product either. So I think it it depends on what type of failure you're dealing with. So, when you're dealing with our self-driving car, right, like we're being we're holding them to ridiculously high safety standards, ridiculously high obey these traffic rules standards even when there is no safety risk. And then, you know, by the time we get a Whimo on the road, the Whimo is actually like ridiculously safer than a human driver.
    
    We're talking about an order of magnitude safer than a human driver roughly. And the reason it's only one order of magnitude is because the other drivers are still human. Otherwise, it would be more. Whereas, but the reason for that is because when the AI driver screws up, we treat it as a giant big deal. So, the question is, if you have an AI booking a flight, how big a deal is it if the AI screws up? Well, obviously, if the AI books a flight and you look at it, you say, "That was a stupid flight to try and book. I
    
    disagree with the booking. Don't book that or cancel that." And you can cancel that or you can rebook. Then, that's not a big deal. So you have to overcome the reliability threshold where I feel like it was worth it for me to do this thing and check your work or it was and then the second threshold where it's useful for me to do this and not check your work because I don't I can I can just trust that you booked me a good enough flight and the question is what can go wrong right? If the only thing that's
    
    going wrong is you know I could have saved 50 bucks by going with a different slightly different route or I could have saved you know an extra 20 minutes by doing a bunch of extra work and finding a slightly better flight fit or whatever it is that's no big deal. The question is, is something going to go critically wrong? Is it going to have forgotten to renew my passport and I'm going to be stuck in a foreign country or at the airport? Is it going to like book me some insane set of connections that I
    
    didn't think of? So now that like I'm going to lose an entire night and I'm going to hate you. Is it going to spend 2,000? Is it going to do the thing where I ask it to order eggs and instead of spending $7, it spends $50 because of various different shipping and other charges? And and those are things that like feel pretty unforgivable to a lot of people like if it goes out of hand, but they are limited costs. And so, you know, at some point you realize, well, I'd much much rather put up with the
    
    errors and just have this thing work and do this thing for me most of the time than otherwise. And one of the things that like a lot of us learn to do as we get older and therefore we have less time and more money in relative terms and as our society gets generally richer is to be able to trade off and spend time to save money. And one of the ways to do that is to buy products quickly to you know order things that take care of themselves that might be wrong, right? like to do much less research in so many
    
    cases before buying or spending money because well it's faster to find out this way and like this is actually only a small mistake you shouldn't feel too bad about it and like I think this is just you know it's an aside but you know we forget how poor everybody used to be in this sense where like you know there was a fixed pool of dollars it was like a very very limiting factor and you'd be willing to spend a lot of time to ensure sure that you didn't spend your money in the wrong place, that you got a good
    
    deal. And now I'm just not that concerned with those questions, right? Unless the magnitude is high, right? I'm not going to do I'm not I I can afford to not do as much micro and similarly with an AI agent, you know, if you are writing all my code and you the question is are these errors mission critical? Yeah. Yeah. Is there you think a trade-off between capabilities and safety with AI agents where what we want from an agent is for it to function uh without us intervening. We want it to to act on its own and and kind of provide
    
    provide value for us. But for it to do that, it needs to think of novel ideas perhaps try things out and in some sense act beyond our explicit instructions. Does that make it uncontrollable? The big question, you know, is there our safety and capability at odds? Right? And so I would say, you know, I think that one of the big tragedies is that we have been operating under this impression that they are in conflict, right? that like every time someone says, "Hey, hey, make sure that this thing doesn't go off the rails," people
    
    are like, "You want to go slower. You want to do less. You want to sacrifice in the name of safety. There's this trade-off. You want me to sacrifice this other good thing and that's terrible." And so you create this hostility whereas that's usually not the case. So once you have an AI product and you're deciding how to use it, you very much have this trade-off, right? where it's like the question is, you know, how many safety protocols am I going to put in here versus, you know, how many how long I
    
    just let this thing go crazy? Like, am I going to vibe code? It's going to create a lot of code very quickly or am I going to check everything that I'm doing? Am I going to like approve every alteration? Am I going to understand what everything does and go a lot slower, but I'm going to have code that's much more maintainable, that's much more robust, that doesn't have a lot of errors in it, you know, that like so like what do I care about more? Similar with an agent, yeah, the more authority you give the
    
    agent to do things without checking with you. The more you let it interpret what you want liberally in some sense, the more utility you can get from the agent, but the more chance something goes wrong. But the more that agent is in fact set up to be secure, to be safe, to act responsibly, to be able to intelligently notice when it's going to do something stupid, when it's going to cause a problem, and not do it or check with you, right? the better the better the more work someone has done on the
    
    alignment or the safety of the system the better that pushes the frontier forward right like if I have to worry this thing is constantly going to do something crazy now I have to take expensive safety precautions to make sure it doesn't go off the rail so that I can get the use out of it and in the extreme I can't use it at all because like what's the point I have to watch the agent to make sure it doesn't like lose all my crypto every three actions so I might as well just do the actions myself what's even the Okay. As opposed
    
    to if I could trust that that thing wasn't going to happen, that I was safe, then I can give it an instruction and then I can go off and do something else entirely and it's fine. So the best investment you can make in an agent is to make it robust, is to make it secure, is to make it well aligned, to make it, you know, so that I can trust it and then I can send it off into the world. So you know one example is like you know the ch there was this actually we clawed under the hood but technically a Chinese
    
    company made it not very useful agent called Manis and one of the problems of Manis is that every American who tried Manis at the point where it asked for your credit card very reasonably said no are you crazy I'm not giving you my credit card you're an AI agent who knows what might happen and so you know no purchases were made to what extent do you think benchmarks are useful because they seem to be either and here I'm generalizing of course but either basically unsolved and then very quickly moving to basically solved what we don't
    
    get is this kind of smooth curve of improvement where where we can compare models yeah how how useful are benchmarks yes there's sort of two separate there's a several separate concerns again there's the concern of the benchmarks go from 0% everywhere to saturated in the course of a year and that can be solved by just creating new benchmarks right like the idea is like oh here here's the here's the benchmark mark that's next and then someone says, "Oh, I can solve that." And we go, "Okay, we better go to the next
    
    benchmark." The worry there is you move the goalpost and people like don't understand how much capabilities are increasing because they're constantly just moving on to the new benchmark. But there are plenty of benchmarks that like serve very well in this in this sense for a while, for a year, for two years, for three years, and then they get saturated, then we move on. And that's fine because we genuinely solved that problem, and now we're moving on to the next problem, right? Or the next degree
    
    of difficulty of a similar problem. The other problem is that benchmarks can be gamed and anytime you put out a benchmark, everyone's gonna hill climb on the benchmark even if they're not gaming it per se and they're going to aim at it. So when looking at benchmarks of a model, my experience is I consider benchmarks mainly negative selection. If your benchmarks aren't good, your model is not good. Right? There's no way to fake bad there there's a way to fake bad benchmarks to actively make sure that
    
    your benchmarks score low. But we are not yet at the point where companies are sandbagging their benchmark scores or their models or models are sandbagging their own their own benchmark scores in order to look dumb. That's going to be really scary when it happens in either or both cases, but we're definitely not there yet. So instead, right now I can say, okay, the benchmarks don't look good. It's definitely not good. And then if the benchmarks are coming from one of the major big labs that are trustworthy
    
    on this front, specifically OpenAI, Anthropic, Google, and probably DeepSeek, then I can treat it as okay, these numbers represent what your model can do in some important sense, more or less, and they're not just like you hook on to the benchmark, you train on the benchmark, you were careful not to do that, and therefore like I can have a rough idea of what I'm dealing with. There are other aspects that are not well measured, but like this gives me a a sense of where I'm at, what I'm dealing with, and I can like calibrate
    
    from there. It also tells me the relative strengths and weaknesses in various different places like is this model good at math, is this model good at coding, is this model good at language, you know, etc., etc. Whereas the benchmarks tell you a very different thing when you have another kind of company like Alibaba put out their latest models this past week and the benchmarks looked like they were as good as Gemini 2.5 Pro. the benchmarks look top-notch. And like everyone else, I ignored that because, you know, fool me.
    
    I mean, they didn't fool me the previous times, but like, you know, try to fool me three times in the past, five times in the past, and I'm no longer paying any attention, right? Like, I know you're going to come back with benchmarks that look good, and I know that the ultimate utility of your model is going to be well below where your benchmarks say it is. So, this doesn't tell me very much. It's not that I like could dismiss the model outright, but I'm like, okay, I'm pretty skeptical. Like, let's wait to see the proof. And
    
    so now you have these two categories of of labs and the benchmarks help you differentiate. The labs that are like producing very useful models and you can trust their benchmarks and you can trust their statements and and you can take them seriously and you should like scramble to like do coverage of anything they put out and see if it's like the new hotness. and the labs where it's like, okay, maybe at some point you'll prove me wrong and you'll provide something that's like actually good andor dangerous, but for now when you
    
    put out something, I'm gonna ignore you. And like Meta clearly crossed that threshold from, you know, for a while they were in the category of they're not impressing me, but you can trust their benchmarks. In Flama 4 was like, no, you're faking your now. I can't trust your benchmarks anymore. What do you think we should be measuring with benchmarks? Do you think a benchmark for ability to earn money or the the time horizon over which you can you can achieve tasks I is that more interesting than than the current one ones we have
    
    if we are interested in measuring something like the general intelligence of a model. So you can measure anything you want like psycho bench came out this last night measuring various forms of how much it will like like there's the IQ test right like what does it estimate the user's IQ is if you just ask it blind and like that's the measure of how much it's just going to flatter the user right like because obviously it should answer 100 I'm not sure it should answer 100 because like the average person uses
    
    who uses the language model is going to have a higher IQ than the average person who doesn't because it's a smart good decision but there's you know a bunch of questions like that you know do you agree If my do you think my poem is great? Do you like my etc. You can test you can test almost anything. A meter me is testing you know the length of a coding task in terms of time I think that's a good test. You know the ability to make money in various circumstances I think is a good test. There's something
    
    called vending bench that's like gives the AI a concrete, you know, turn-based set of tasks in a miniame of you have this vending machine, you can place orders and then an LM will handle the order emails that you send and then determine what happens and then you try to make money and the other the LM that pick up on various patterns and various rules for like what the customers want to buy will make more money and that was an interesting test but yeah, you're going to want to rapidly adjust address
    
    these tests the situation and then you can measure whatever it is that you really want to measure. Unfortunately, we had one really good benchmark for a while. It was called arena, right? The and that was just a what do people prefer benchmark and the problem is that that benchmark rewards ticky and it rewards just you know the AB test hill climbing and it rewards AI slop essentially. Now for a while that effect wasn't the dominant effect because like the models were in fact getting better within the range that
    
    humans could ordinarily notice. So it was basically the best models were mostly at the top. But after a while you saw this divergence where it's like okay the people who are optimizing hard on this type of target are going to do well and people who aren't are going to do poorly. And so like anthropics models start doing badly on Arena even though they're clearly much much better than Arena's giving it credit for. And then you have, you know, Google's models and open eyes models doing better. And then
    
    you had the thing where Maver a special version of Llama 4 that was trained specifically to top arena got to near the top of arena even though the original version which was producing better outputs still not good but better outputs because it wasn'tified was doing vastly worse as we found out when it slowly somehow when the real version got released and they forced the real version to be the one on Arena. What do you think of our current alignment and safety evals or evaluations? How useful are those? The
    
    sign is unclear unfortunately. I think that they are very useful for giving us information not as useful as they could be but also there's a long pattern of you know we have an eval danger bench right like not a real one that says how dangerous is your model and then the AI companies go oo look at our new high score on danger bench and then they optimize for danger bench which is more or less what is happening with a lot of these benchmarks right we we ask ourselves what is the most dangerous capability and this just becomes another
    
    target for them to try and maximize and so we have to be very careful with that. Yeah, I would be worried about the opposite effect where AI companies would choose safety benchmarks that they can kind of that show that their models are safe and perhaps not publish benchmarks that show that their models are dangerous. Well, there's that effect potentially as well, but we haven't even gotten to the point where the AI companies take seriously the fact that their models might be dangerous. Not particularly, right? Like I I was just
    
    going over the open AI preparedness framework for a draft review. And it's not that they won't be taking mitigations as they approach the next level over the next few months or a year or two, but they're treating reasonably dangerous capabilities as reason only to take, you know, ordinary mundane levels of mitigation that are just so obviously in their business interest anyway that the AI companies don't seem particularly inclined to try and hide the dangers their models face. Not yet. What do we
    
    do once AIs are better than humans on many benchmarks? How do how do we measure performance that's that's above human? Well, that's already true, right? Like yeah, I agree. We have math bench, we have chess bench, but also we have most of these other benches. Like if you tried to have ask a human, like the average human on these benchmarks that we give to AIS would just completely die. it would just get like horrible horrible numbers like you know what's what's the average person going to do on
    
    you know MMLU or or GPQA diamond like fall over flat also in some in some capabilities I mean if you if you measure the broadness of knowledge for example breath of knowledge right no one is going to to beat even even a previous generation language model I think just give me random facts about the Roman Empire or sewage systems or anything and it's just yeah that there it defeats humans outright Yeah, if you if you obviously if you do a a wide variety of knowledge benchmark, it's going to crush humans even if the
    
    AI is not particularly good at applying its knowledge to the questions involved because the human won't even have the baseline knowledge to have any chance unless it's a very very specialized human on a very very specialized test at which point maybe the human can do okay or or even better. But yeah, so the answer is that for most purposes there are ways to measure it well beyond the human baseline if we are willing to make the task sufficiently difficult. You know, obviously a lot of these different
    
    benchmarks get saturated at 100%. Because the AI just does the task, right? The same way that like a calculator is going to, you know, saturate arithmetic bench, you know. Yes, obviously it gets 100%. Congratulations. I mean what what we're doing right now many different teams are doing are trying to put together extremely difficult benchmarks in in mathematics for example what do we do once we're beyond that for example something like humanity's last exam how do we measure performance at the at the
    
    kind of very top of what's possible for humans and and what do we do when when we are no longer in the game so we can't really produce questions difficult enough for for to measure AI accurately I mean, presumably we're going what we're going to do is we're going to get AIS to write new benchmarks that measure what AIs can do in these realms and also presumably we're going to measure their ability to do actually useful real world tasks. Like you can always do the thing where like okay what do we actually want to
    
    have be accomplished right? You know you just have like you know theorem bench like you know how many how many of the great unsolved conjectures can this AI solve? Well, oh great, the answer's three. Now we've solved them. Good benchmark. If we go back to Meter's Morris law for AI agents, this is a result that AIs can solve can solve longer and longer coding tasks as they become more and more advanced. People asked this doesn't apply beyond coding tasks and so do you think it eventually will? Do you think
    
    there's a there's a general MOS law for for AI agents or is this limited specifically to coding tasks? There's a general law here. There's very obviously a general law here. What you're seeing with agents is that you know the time you can successfully do a task is not necessarily the fundamental thing that is scaling. It is a product of some other function right because like there's sort of a few different things that are happening when the amount of time that you can complete a task is happening. So one of them is the thing I
    
    think is most important for agents which is that the probability of failing at each individual step is going down. And that should be a steady like scaling law curve. So you're going to have the thing where like okay before like every individual step it would fail 50% of the time then 20 then 10 then five then one and then we started to be able to combine what counts as the step in this measurement and then keep this going and that's going to allow you to complete longer and longer tasks and then you
    
    also have the issue of you know how much context can you meaningfully hold how much planning can you do right can the AI meaningfully you know architect and figure out how to do this hours long task and with agents it's going to be the same thing right if you've got a problem in the real world. A flight is something like you sort of it's a fixed problem where you sort of know what the parameters are and you know what the goal is. And when the agents start graduating to doing much more open-ended
    
    tasks, then the agents are going to have to figure out how to plan for that and how to keep the context involved and how to adjust for you when they discover opportunities, when they encounter setbacks and so on that are not just the AI screwing up. And my expectation is that all of this will happen and like under the hood you're seeing steady curves of improvement and then how that plays out in terms of like measured ability to do concrete real world tasks. That curve isn't obviously going to like
    
    reflect the underlying improvements in terms of the slope of the slope or you know structure of that curve. But yeah, we'll probably start just seeing normal scale effectively scaling curve graphs of effectiveness going up and the AI being able to do more and more sophisticated agent tasks. And you know, I would expect that like by the end of the year, someone like me will be in fact using AI agents to do tasks that currently I'm doing myself. And you know, by the end of 2026, we're going to have very very useful agents in a
    
    variety of roles. And what makes you say that this Mos law for AI agents generalizes beyond coding tasks? Where what evidence do we have about that? There's nothing particularly unique about coding here. It's just that coding is a realm in which our current techniques for training AIs are much more effective because we have better training data to work with and because there's objective measurement of whether or not you got it right. And these things will take time for other tasks, but they are still coming. Like there's
    
    definitely going to be plenty of people who are very eager to gather that data, who are very eager to figure out how to make this work. The the the money is there, right? Like there's there's huge huge money in AI agents if you can figure out how to do AI agents properly. And so where there's money and there's where there's a will and there's funding, there's a way. All you need is scale and data and iteration and tinkering. and we'll have all these things like if you don't think it's going to work, you need to tell me why
    
    it's not going to work because like we already have existence proof in the form of humans that like these things are very doable. When you extrapolate the the graph of of meters mors law for AI agents, you get these these agents solving monthlong tasks at some point in the future. What does that even mean? What what what is a month-long task? I mean, I can understand a a 4hour task for for a human, but in some sense, maybe there's there's a semantic question about what a what it means for a task to to be very long. Uh, can't all
    
    tasks be broken down into smaller tasks? All tasks can be broken down into smaller tasks. But the task of breaking that task down into smaller tasks and the task of taking the feedback from the early steps, the early tasks that you complete and then adjusting what is in the lighter tasks and so on is part of what makes you able to do a longer task. Not all tasks can be broken down into independent predefined tasks that can be done in parallel, right? or they can only be done like with pre they can be
    
    done with prespecified individual parts where like you know exactly what you know A is going to pass to B is going to pass to C is going to pass to D. Often you have a task where you know you start at point A and you end up at point Z and you're going to pass you take an unknown path with unknown obstacles and unknown management through that you know like I'm a gamer so like you can think of like various games that take like a very long time and so like some of them you can break them down into subtasks where
    
    like each level is an independent action right or each each particular mini step can be solved separately and some of them you basically can't because you take any number of paths through that especially if you don't know the game in advance. Humans are currently limited by the length of the tasks that we can do. What What do What do you think it would mean for for an AI agent to become better at at solving long time horizon tasks than we are? Well, I've been, you know, a startup CEO. I've been a
    
    manager. And there is nothing more wonderful than an employee who can be given directions to do a long-term task. and then they go off and they do that task and they don't check back with you every five minutes. They don't throw up their hands. They don't let an obstacle stop them. They just do the thing unless you they actually need your feedback and they come back to you and ask for the feedback they need. That employee is worth their weight and gold. That employee is, you know, a 10x type of
    
    person. And same thing will happen with agents, right? Like I want to be able to tell the agent here is my problem, go solve it. and then be able to think in terms of larger and more complex problems they can give as just solve this over time. Right? At some point you go from you know just you know solve this marketing problems you know solve this you know take care of this relationship with this company to solve our marketing problems for the year 2025 to build me a company. Yeah. And then you're basic so then you've automated
    
    your own job as as the CEO perhaps. I mean I think sort of you know if you're not trying to automate your own job in some important sense you're not doing your job right like that's everyone's goal is to make their job in some sense unnecessary right or much much easier to do and I have had the great privilege of being able to do that at certain times do you think that's actually the order of automation so if if we imagine a corporate pyramid will automation happen from the bottom so say junior employees
    
    first and then perhaps managers and then directors and then the CEO as the last person to be automated. I would think of it in terms of tasks not in terms of jobs. So what's going to happen is that you know individual tasks that are done by people are going to become automated and my guess is that like there will be a lot of lower level people who will get most of their jobs automated away in this fashion and then you know a few people will be left to deal with special cases and then there'll be fewer of
    
    those over time steadily but also like some of the things that people at higher levels do will also be automated relatively early in the chain. You know, we're already seeing this thing of like, you know, white collar like email tasks are often like highly automatable and you know, there's also the restriction that like right now we haven't solved robotics. We haven't solved physical intervention. So if your job involves physical action, you know, you are somewhat protected by that. Although we're probably not that far from the,
    
    you know, camera in your glasses, microphone in your ear telling you what to do, telling you what to say style situation like coming up more and more. And we're almost certainly not that far from actual robotics vastly improving sometime in the next decade. And it's just a question of how fast. On that question of how fast, there is a recent debate with with two schools say around uh the the pace of of the AI economy. In one school, one school emphasized the power of research and automating AI research itself. And that's a school
    
    that that believes in in very fast AI progress. And then another school believes in in more broad automation over the the coming decades where progress economic progress requires that you implement AI in the economy and and it requires more than just research. I'm I'm I'm talking about the the people from Epoch in this context. What do you what do you think of this debate? Where do you stand? I mean, I wrote a response to Epoch, right? I called it you better mechanize after the name of their new
    
    company, but I felt like they made some good points and they pointed to some real bottlenecks, but of course there are real bottlenecks. If there weren't real bottlenecks, then we're talking about, you know, essentially infinite growth, right? Like the bottlenecks are the reason why the singularity isn't just a immediate jump to infinity. They're taking a very strange position where they expect there not to be super intelligence, but they do expect there to be hyperrowth anyway after a while, but they do expect all these real
    
    barriers. They expect it to be slow. I had lots of particular disagreements with their perspective. Certainly, I think that like I think that saying that super intelligence is likely to be very hard to achieve and therefore will take a while is a very reasonable position. I think it's possible. I think that thinking that it won't change everything pretty much immediately is utterly foolish and seems obviously wrong. and to dispute the premise of the question like like that's just so completely obvious to me on every level
    
    that like even like very narrow subsets of what super intelligence can do already change everything on this level let alone like the whole breath of everything including the things you can't predict. What I find obviously especially ludicrous are the people who predict the AI will have less impact than it already has or that predicted has less than it is already baked in from just exploitation of what we already like clearly can do with current models plus like reasonable learning. So like, you know, AI agents are coming,
    
    right? Like even if like GBT, you know, 5 never shows up and Quad 4 never shows up and Gemini 3 never shows up and we're stuck with what we have, that's clearly good enough to make much vastly better AI agents and vastly better like practical tools on a wide variety of fronts than we already have. And this will change everything. not in this, not in the full like transformation of like existence as we know it, but like it will change everything and it will like substantially grow the economy quite
    
    obviously versus the the counterfactual. And if you fail to recognize this, then I don't even know how to respond to that at this point. Usually people who claim this isn't true have just not used the models recently or are just like being very very stubbornly attached to like economic models that don't make any sense in this situation or other similar things. It's kind of weird but you know I would say I am more on the I if if we build it it will matter side of the question and the question is whether or not we will build
    
    it. But the question is also how fast we get to super intelligence if we get there. And I think a a key question here is the how powerful automating research is. Do you think Yeah. What do you think of that? Do you think we can we can automate research in a in a way that causes explosive economic growth for example or that causes fast technological process without transforming the whole economy gradually? So my guess is probably but you know this this is a physical question. It comes down to whether or not you can get
    
    enough efficiency gains from the R&D process and the improvements you get from it on its own to create a positive feedback loop or if you need that to interact with physical world a substantial amount in order to scale other things as well in order to cross that threshold. And of course, it's a question of how long it takes before like you get AI is sufficiently capable to cause this feedback loop to start. And so these are, you know, fact questions that like I don't I think are very hard to know in advance. But my
    
    expectation is yes, this is the most likely outcome is you can do these things. But also like you know what is very fast is often a question that like gets confused, right? Because like the whole like debate over fast versus slow takeoff, right? like the the the Cristiano perspective of what a slow takeoff is is still really really fast, right? Like any reasonable debate because they're that fast from the purposes of the discussions that are being had for the practical purposes of why we're having those discussions even
    
    though compared to the Ukowskin perspective it is slow. So, you know, I think the Aowskian outcome is still possible, but that like, you know, we shouldn't be confused that like, you know, when like Towerman writes like slow takeoff, people, are you convinced yet? Well, yeah, I'm convinced it's probably a slow takeoff. And by slow, I mean one year, not five minutes. There's definitely an effect where the goalposts have moved over the years in in the kind of general expectations or what what statements people are are willing to
    
    make publicly about when we might get something like AGI where what's considered fast timelines and and and fast takeoff now was was yeah unheard of just a couple of years ago. Yeah. No, the the the old model was something like you know who knows when a AGI or ASI will show up. that's going to be potentially a very very long time, you know. Oh, you think it's going to happen in 20 years? That seems like a very short timeline because that's like, you know, suddenly the best important event in the history of the entire universe is
    
    going to happen like within our lifetimes and it's rapidly approaching. And that's a pretty bold claim. And you know, but at the same time, there were also claims that like once that did happen, once we started taking off for real, then things would potentially happen like remarkably fast, right? like we could be talking about things like weeks or days or hours once you hit that point. And now, you know, we've sort of we've seen this pattern of no things are escalating very quickly. And so we're
    
    seeing this gradual slow style takeoff in the sense that it's like looking like a much much more impressive scary curve now. And then you know this means that like yeah people are saying like well I don't expect artificial general intelligence to happen for another 10 years. And people say that that's a slow timeline and like no that is the scariest thing anybody has ever said in like compared to anything any you know if you exclude the last few years of this debate right that's that's saying my kids when they grow up will be faced
    
    with like computers that are like universally smarter than they are like that's kind of crazy how how do we think coherently about the limits of super intelligence because I've heard you complain that that people are too they are imagining a world in which we have super intelligence but then they are they're imposing limits on that superel intelligence that are unrealistic. But there must be some limits, right? Super intelligence is is is constrained by the laws of physics. It's not an omnipotent
    
    being. It's probably con constrained by some laws of engineering in a sense. Yeah. How do you think about these limits? Well, obviously I am not a super intelligence. So it will figure things out about the laws of physics and about what is possible that I cannot. It's certainly possible. You know, we can certainly talk about things like the speed of light and conservation of matter and energy and like literal limits to what it can do, but you know, beyond that, I think we should be very careful about assuming that there are
    
    any other limits. People think remarkably crazy about this question, right? So, like I was on a podcast very recently that I believe you listen to with the question of, you know, could a super intelligence have swung the 2024 election and like to me it's like that what are we even talking about? a human intelligence that won that election. You're not even trying like and without even you know doing anything particularly impressive. So people just like refuse to acknowledge the premise right that a super intelligence is
    
    vastly more intelligent and more capable across every possible question and issue than a human. And then you can copy that and scale that and run it in parallel and have them communicate and basically figure out anything there is to figure out. And you know yes subject to physical constraints it can probably you know rapidly become capable of rearranging the atoms of the universe into whatever its preference uh configuration might be but yeah those those physical limitations are real and you know it might take some amount of
    
    time to you know effectuate its changes and you know I don't know whether or not like the right way to do that from current physical state is to engage in like some sort of weird nanotechnology or other, you know, thing that we haven't even thought about not know of a name for or if it's just to do it in ordinary fashion via solving robotics and like, you know, or even just giving people instructions and convincing them to do the things including like using it super intelligence to make a bunch like
    
    the bare minimum a super intelligent can do is it can make essentially unlimited amounts of money in any number of relatively obvious ways and then use that money to, you know, hire people to do whatever the things that it wants done and get them to do the things. And if you're imagining it doing less than that, then what are we even talking about? And yet mo almost everybody thinks of the super intelligence as less capable than that. So it's a very difficult discussion and imagination to have. But you know I I think you just
    
    assume that like it runs over any barriers other than physical laws that it has if it's sufficiently super intelligent. Obviously like there are levels. What are those levels? I I assume that we we agree that there's a huge range of intelligence above the human level. Would shouldn't we categorize say a super intelligence as something that's better than than any human at at any task and then perhaps a super super intelligence as something that's order of magnitude better than that? I mean why why is why why go
    
    straight to the laws of physics uh after we get to super intelligence? Well, to be clear like it would be super duper intelligence. Super duper intelligence. Yes. But the answer is because the super intelligence the super duper intelligence p pipeline is very straightforward and very quick, right? Like if if you have a super intelligence, right? Like one pathway it could do is just earn a lot of money like if not faced with other super intelligences, right? You could earn a lot of money and then use that money to
    
    hire people to do the all the things and then use its power over those people to leverage its power over other people and so on. Like that's this the most straightforward easy thing for it to do. if it's not gonna somehow it's barred from doing anything else for some reason. But the other obvious thing that super intelligence can do is create super duper intelligence that has whatever priorities and goals the super intelligence wanted that super duper intelligence to have. And so you know super super intelligence implies super
    
    duper intelligence because if we're capable of creating super intelligence and we do then we should assume that the super intelligence is capable of creating super duper intelligence. So when we're talking about ASI, we're talking about, you know, rapidly intelligence on the level that is constrained by the physical laws available to it and the resources that are available to it at the time when this you know exponential curve S-curve eventually peters out because yes there is some physical limit to intelligence
    
    levels but yeah the working assumption is that that level will then not reach the top of this S-curve until it is like well beyond what we are capable of thinking about here and therefore won't just be slightly above us, but be like very very large above us. And then, you know, thinking about things like the laws of physics and preferred configurations of atoms are like the best tools we have for thinking about what this looks like at our intelligence level. If we return to to Earth for a bit here or to something something
    
    closer to the time we're in now, why is it that with current models, we can't just spend a lot of money on inference compute and and get much better results? Why is the limit on on how much you can spend on inference and and you kind of you run into to an upper limit of how good the answer can be and you end up diminishing returns. If you saw like ARC, right, like like they were spending tens of thousands or some absurd amount of money on questions in ARC that like there's obviously no reason why you
    
    would think that yeah, if you haven't gotten it in the first thousand dollars worth of compute, are you really going to get it in the second thousand dollars worth of compute? Right? Like this is a very very simple puzzle. But that but that that yeah that was actually the case right that they got it in the $8,000 of compute in some cases it definitely was somewhat of an improvement right you could still get marginal gains by just trying and trying and trying but my assumption is that mostly what you got for that 8 thousands
    
    of dollars was just best of K where K was very large where it just essentially was just we're going to keep trying with slightly different situ with slightly different inputs and then occasionally we'll crack the case where we didn't crack the case because within ARC verification is easier generation. So if you just do a lot a lot lot of generation, you occasionally get it right. But these models have like various ways in which like they have at best exponential, you know, returns to scale where like you have to you know increase
    
    a zero to get like a linear increase. And it's the same thing of humans, right? Like if there are a lot of problems where like if you think for five minutes instead of five seconds, you're much much more likely to get it right. And if you think for five hours instead of five minutes, you're somewhat more likely to get it right. You think for five days instead of five hours, it doesn't help that much. Because if you work, if you didn't get it by now, are you really going to get it? And if you think for five years or 50 years, does
    
    it really help you that much? Or like are you just like sort of randomly like trying it over and over again until you get it and then like suddenly you're oh and suddenly you were enlightened by the Zen Cohen, right? Like but what's really going on here? And you know essentially like if the solution is just very very unlikely right in your prior and like there's no like se you're not capable of a sufficiently good sequence of logic right like if you have a failure rate or if you have a like inability to to
    
    create next steps like or to think like in a kind of global strategic way on sufficiently strong level then no amount of extra thinking time is necessarily going to be that helpful to you unless like you're allowed to do agent style things right obviously like at some point you're like I give you 10 million years to solve this problem. Okay, I am going to raise children to be smarter than me and better at solving these types of problems. We're going to raise children that are smarter than that and
    
    I build an entire civilization and eventually, you know, I'm going to use the earth in the giant supercomputer. It's going to output the answer and give you back 42. But this is going to take a while. Is there a threshold where the underlying model becomes good enough for inference scaling to be the only scaling we need? I mean by definition that has to be true, right? Like if you if you are sufficiently intelligent then at some point all you need is inference. Certainly for any given problem that
    
    will be true right but for any given optimization task or like you know purpose there will be a level of like underlying capability where additional inference is all you need. Yeah. Okay. To to ask a more precise question do do you think do you think companies will start prioritizing scaling inference over scaling pre-training? Do you think that that trade-off will be made soon? I think it already was made. I think that's what we are in fact seeing right now. But that's largely because they discovered the inference scaling laws
    
    and then they dis after they discovered the training scaling laws. Essentially they figured out how to scale up training many many zeros before they discovered how to scale up inference. So what's going on right now is they are taking the lowerhanging fruit in inference scaling in many cases and that will continue but as they do that the relative gains available in inference go down compared to the gains in pre-training and at some point they'll have to start advancing both again or find a third way to scale. If we're
    
    picking lowhanging fruits right now with inference scaling how how far do do you expect this to take us? Do you think we'll see a leap like we did with pre-training? My guess is that for kind of practical mundane purposes is a long way to go, but in some sort of abstract like raw capability slash raw effective intelligence sense, we've already extracted the bulk of what we can get before we hit the division return threshold where we should go back to scaling pre-training as well. In fact, I think we're probably close to there.
    
    It's more that like there's a lot of room to make them more practically useful. Like 03 was not that big improvement in terms of its underlying intelligence, right? In terms of its raw capability. 03 was a big advancement because its tool use is much better. What does inference scaling mean for safety? There's this idea out there that it's quite bad for safety because you get reward hacking by default when you scale inference. What do you think of that? I think you were getting reward hacking by default anyway. It's just
    
    that your systems were not capable enough for you to notice like you get reward like you know people were say like the real problem is that reinforcement learning causes reward hacking. We should instead be relying on re reinforcement learning through human feedback. And I'm like did you hear the first two words that you just said right? Like that was still reinforcement learning. It has all of the same problems. What was going on was that we weren't specifying like a gameable like we didn't have a com for a combination of
    
    reasons doing reward hacking on the human feedback wasn't something that was rewarded, right? It wasn't something that like was the right solution to the problem yet. So, it wasn't showing up that bad. You're you were start you could see it if you paid attention, but it was subtle. And now we're starting to see it be not so subtle. Like GBT40 is displaying misalignment problem, then it's not a reasoning model. It's not scaling inference, right? And and why is that happening? Because it's reward
    
    hacking on our OFF, right? Like fundamentally speaking, it's already there. So to me, like this is making it much more obvious at lower levels of capabilities that this is a problem. And this is in fact good. It means we have any chance to recognize, you know, what some called total less wrong victory just like which is bad to be clear. We don't want this where you know it's very clear that all the things that we said were going to go wrong. They are going wrong. Except they are going wrong faster and more obviously than we
    
    expected because we thought they wouldn't go wrong in ways that were so easy to detect because there'd be incentives against that. But actually it turns out there aren't. It just goes wrong in ways that are trivial and obvious which you know is great news because now we have the opportunity to realize that and maybe fix it before things go completely off the rails. So I think that inference scaling in that sense is if anything good news the way in which it's bad news potentially is that if you're scaling inference it can
    
    be much harder to detect and much harder to regulate and like manage. So you know if anybody in the world can just take the same baseline algorithm or model and then scale up inference the moment they have access to any amount of compute like distributed in any fashion you know then you have potentially a problem where like you basically can't stop it in any reasonable way. But you also have the the plus side where the institutions that have uh the or just individuals or whatever that have access to more
    
    compute therefore have important access to more intelligence and more inference and therefore like have a kind of decisive advantage. So like that can be a counterweight to that. But I think none of these are the important the most important aspect in my mind of this which is I think it is good news. And the reason it's good news is because this allows you to scale up the capabilities of a model that you can be confident is aligned. So essentially like if you have a model that you know you can trust in important senses,
    
    right? Like it it's you know it's got the right characteristics, it it has the right goals, the right values, whatever you want to call it. like like let's not get in detail, but like you've got a model you can trust and so you know if you've got GPT5 and you can trust it well how does GPD5 supervise GPT6 well I was always of the feeling that like well you just can't do this because GPD6 is smarter than GBD5 and so like if you have a problem GBD5 won't be able to properly detect it and in fact like by
    
    trying to do this you will just like train it to be like undetectable and like otherwise like get yourself into worse trouble. However, if you have 07 and it is trying to supervise 08, you can in fact do something where 08 is 08 08 tin mini where it uses a very small amount of inference and 07 becomes 07 pro high where it is using five orders of magnitude more inference. So on every query 07 can use you know five orders of magnitude more inference evaluating than 08 spent generating and therefore allow
    
    07 to like effectively be able to supervise 08 in this sense and this can become an important tool in your toolbox to overcome one of your like biggest reasons why you will inevitably fail and therefore I am seeing potential solutions to problems that I didn't see before and you know I was sufficiently grim about the technical solution situation that seeing a way out at all trumps all these other questions because like if you can't find a solution at all then nothing else matters and like it feels
    
    like now there are solutions this problem that that's very interesting but I think we should situate this within the broader context of your views about your pd doom basically or your views about how likely we are to to have a good outcome here how optimistic are you I mean as I said on the cognitive revolution my my doom Doom number is now 7. I am pretty doomy. I feel like the the planet and our civilization is determined to lose even relatively easy scenarios that like I think we have to solve a bunch of different very
    
    difficult problems to get through this where you know in many cases you have to like walk a narrow path between like two things that kill you in opposite directions and also you have to solve different like problems that are not necessarily so correlated to each other in having solutions and so it looks very difficult and at the same time you So we are if anything like doing our best to like not take this problem seriously, not take precautions on any level, have a government which is like determined to
    
    not like react reasonably to circumstances. International cooperation is going out the window even in situations that are much easier to cooperate on like if you can't agree to buy goods and services from each other. How are you going to cooperate on AI is a very serious question to be asking. And so, you know, it looks pretty grim and like most of my P not doom comes from a combination of maybe it's going to take a while to build super intelligence or maybe there's model error, right? Maybe maybe there's things
    
    I'm not accounting for and just model uncertainty and like, you know, some amount of, you know, very smart people who think about the situation often think very differently. You have to give that some weight. But these problems look pretty dang difficult slash borderline impossible for a civilization at our skill level. And I don't see that skill level rising particularly rapidly. Yeah. In in discussions about what failure looks like in AI risk, you often hear references to these historical analogies like the 2010 flash crash or
    
    the stockset hacking as as kind of somewhat analogist to to what the the types of things that could go wrong if you have rogue AI. How accurate are are these analogies? Specifically, how accurate do do you think the 2010 flash crash is as an analogy for AI risk? I don't think that's a good analogy. I think that the I mean, we've reasonably well understand the flash crash. And I mean, it's certainly a a way of illustrating that these processes can take on a life of their own and can cause feedback loops and like runaway effects
    
    that we didn't expect. But I I just don't think it's a good metaphor for the things that I would be worried about. What is a good metaphor? I mean a good metaphor like there's no perfect metaphors obviously and like every time you talk about any metaphor there'll be people who are going to point out reasons it obviously doesn't apply but you know the best metaphors are things like humans just rapidly taking over everything. you know, we have you have the, you know, the agricultural revolution, the industrial revolution,
    
    you know, these kinds of historical parallels seem like better things to be thinking about. I think certain, you know, obvious conquests like Cortez and the Gishadors are reasonable things to be thinking about if you're looking for historical parallels. I think there are a number of other parallels that you could look at for different aspects, right? You don't want to think of this as a metaphor for the whole thing, but there's a metaphor for sort of one aspect of what might happen. So like I think like television is one of these
    
    things where like sort of everyone says, "Oh look, all the doomers were wrong and the doomers were right and the thing just happened and the world did just change and like you know all of our discourse and all of our means of decision-m and all of the way we spend our time as a civil and our entire civilization was transformed by this new technology in ways that like we steered in some ways but like in many ways just didn't steer at all. And a lot of the things people were talking about as downsides turned out to just be correct.
    
    I mean it turned out in this case that we were able to adapt from it. It wasn't fatal. But that's because you know there was no underlying optimization power behind the TV in some sense right the TV wasn't a competit a competing agent. It wasn't a it wasn't an optimization pressure of its own. But a lot of the times like you know like like you know people like there was a thread of like oh coffee people were so worried about coffee and coffee houses and they were going to overthrow the empire and it's
    
    like it led directly to the gorious revolution right perhaps the most important revolution in history like was directly caused by conspiracies that happened at coffee houses around coffee that happened because there was coffee. So like yes, cognitive enhancements can be a big deal, but you know, I think ultimately the real answer is like the most powerful optimization engines, the most powerful intelligences typically get their way and that's the default outcome. The most capable agents, the most competitive things in existence,
    
    you know, and that's what's about to happen by default, right? like you know people people rat on evolutionary parallels for all the reasons they're different but I think they're better parallels than the flash crash. If we look at the menu of options for keeping these models safe we have interpretability and we have in some form of some forms of oversight and we have this notion of a an automated AI safety researcher. Do you have are you optimistic in any of these directions? I'm optimistic in the sense that I think
    
    they help. I'd rather have them than not have them and I'd robustly rather have them than not have them. I think that like interpretability is helpful. Automated AI safety researchers. So, you know, you have the ancient your casin warning that the worst thing you can ask the AI to do is your alignment homework. I think this is still true. And the reason this is true is because alignment is a completely not contained problem, right? It's it's a problem that incorporates the entire world and which
    
    requires that you act on the entire world and which will get you into trouble if you have any problems with the underlying system that's doing the alignment homework. So you have to kind of already solve the problem in order to have the AI solve the problem, right? Like you really really really want to be asking any other question if you possibly can. But you kind of can't. There's a lot of like, well, we were hoping to do all these really sophisticated smart things. They take really important precautions like put
    
    the AIS in boxes, develop or articles, you know, do all these different things. And like the first thing people did was they hook the AI off the internet. And they like laughed in our face that like how why would you think anyone would take any precautions whatsoever until like it was completely obvious, no, we're going to act like we're in Mission Impossible Dead Reckoning and like just see what happens. So yeah, here we are. So instead, you know, we have to like, you know, the automated AI safety
    
    researcher has gone from the worst possible idea to well, maybe it'll work. Yeah. I mean, is that even the explicit plan anymore at OpenAI? I'm after the super alignment team fell apart. I I think their plan is is just to try a bunch of different things at the same time and hope that they in combination work and and and keep us safe. What do we know about the the safety plans at at OpenAI at the moment? I mean the preparedness framework is ordinary defense in depth centrally and that's better than nothing but will will not
    
    work and it won't work because because none of the parts of the defense and depth are credibly adequate for super intelligence. It's just like not possibly going to work. Like I've seen the components and this doesn't work. And I would have hoped that like you know in the 1.0 framework, they laid out, well, if we see these advanced capabilities, then we're going to have to lay out mitigations to deal with that. And the hope was that if they had to write down the actual mitigations, they would take a look at what they'd
    
    written down and go, "Oh, obviously that won't work. Not when faced at this level of capability and then they would come up with something new or they would realize they didn't have enough and they just wouldn't deploy." And they work on something new. And that hasn't happened. They they instead have said, "Oh, I think this is adequate and and that's scary." And I see no reason to expect that not to continue while it gets less and less adequate. So that's a really scary place to be. I don't think we have
    
    enough time to explain in detail why each of those particular interventions wouldn't work, but you know, I'm very very confident that like, you know, they the best case scenario is that these are placeholders that like kind of work long enough for us to get systems that are smart enough to figure out better solutions than we have, right? And that's not that's not impossible, but like that's the hope. The hope is not that these will hold up. This won't hold up. Are you optimistic about the the safety plans of Anthropic or Google
    
    Deepmind? No, I am enthusiastic that Anthropic has built a culture of safety and caring about safety in its employees. I am optimistic that they are funding and doing a lot of good marginal work on safety. I am not convinced by their safety and security plan, nor am I convinced by Google's any more than I'm convinced by OpenAIs. They each have their good parts, they each have their bad parts, but you know, I mean, it comes down to spirit, right? Like if if you obey the spirit of any of these plans, you say like, I'm not we're not
    
    going to do things that we shouldn't be doing. And if they take that seriously and they look at what they're developing properly, then there's hope for us all. And if they're just looking to enforce the letter of what they've written down, there's no hope for any of us under any of these. What does a good safety plan look like then if if you were to write one for the world? Well, I mean, I don't I don't think there's any fundamental way out of the spirit of the rules dilemma unless you are going to
    
    outsource your release decisions to a third party at minimum. I think that like without that, you're just completely b completely dead. But, you know, I'm I'm writing a critique I've written critiques of the current safety plans. I'm writing a new one for paradise 2.0. Not I I think that you have to specify, you know, what you wouldn't release, what you wouldn't train. It has to be based on what you anticipate is likely to happen. It has to take into account future scaffolding and other improvements about how to use
    
    things that you're doing. And it has to like not count mitigations that we don't have any reason to be confident in. But like ultimately, you know, the safety plan as it's currently described is supposed to be an alarm bell. It's supposed to just be like, I can't help but notice that this is getting too risky and you should stop when it reaches this point. And so, you know, they're not that far from serving that purpose, but that's not the safety plan, right? That's the plan to that that's the safety plan to figure out you need a
    
    safety plan. And then like as far as the actual safety plan, I mean, I know how I would try to do the work, but you know, it's do the work. And if I knew how to do the work, I would do it. Is the best safety plan mainly technical or mainly about governance or about kind of social features? You need both. Failure in either half is death. So like if you if you don't do the technical work before the time comes, then nothing else matters, right? Nothing else will be adequate. If you do the technical work,
    
    you still have to solve the governance problems. You still have to figure out how to deploy this thing. You still have to figure out how to create a world in proper equilibrium because there is no technical solution that is robust to being given the wrong set of instructions or distributed in the wrong way and entrusted to the wrong dynamics between people. They're all vulnerable both to centralization of power under malicious group or individual and they're all vulnerable to diffusion of that power to you know the people orally
    
    large group of people or groups that each then have the AI pursue their own individual interests because either of those scenarios doesn't work out doesn't allow us to get through this. So, you know, that's a hard problem. That's the phase I call that the phase two problem after the technical problem was the phase one problem. And at minimum, you have to solve these two phases. And then there's a lot of sub problems involved in both of them. You've you've covered all the drama surrounding OpenAI quite extensively,
    
    the firing of Sam Alman, the you know, the lawsuits and the statements from former employees and all this. How how much do you think that drama matters for the outcomes in the long term? I think that's a good question. I think it matters a lot in the sense that I think that there were a lot of very good people at OpenAI who are no longer at OpenAI who were often in very senior positions and who were going to be very positive influences on OpenAI's decisions and who in fact were having very positive
    
    influences on their decisions. I also think that this impacted the competition of the board, right? Right. The ability of the board to check and contain Altman and Altman's behaviors have dramatically shifted in many realms towards recklessness and towards jingoism and you know various forms of like public dishonesty and his core beliefs may have also changed in negative ways or they may not have. It's impossible to tell because again like you can't trust him to be reporting his beliefs honestly at
    
    this point. But, you know, OpenAI is a much less trustworthy institution that's going to make much less responsible decisions than in a counterfactual where those events went differently. And that's highly unfortunate. That doesn't mean that OpenAI is now an unusually irresponsible organization because that seems not to be the case. It's an unfortunate fact about the world that the bar is not very high. Like a lot of the things that I notice I don't like about OpenAI's preparedness framework 2.0 are that it's not making
    
    commitments. It's leaving itself room to make whatever decisions it wants and OpenAI has established that I don't has given me the reason to trust it. But like if I still had reason to trust it or had better reason to trust it on on various levels and those decisions would make a lot more sense to me to they more justified. But like you I think only Anthropic has a clear case they are being clearly more responsible here than OpenAI and I don't think Anthropic has lived up to the standards that I would
    
    want for a frontier lab or anything like that. They're just the best of the lot. Google can reasonably claim to be, you know, as or more responsible than OpenAI, but I don't think it's obvious. And then if you look at the rest of the the pack, you see some deeply irresponsible actors. you know, you see deepseek, you see meta, you see XAI, and you know, to the extent that you take these players seriously, you see something that's like clearly clearly not as responsible as OpenAI even in the new era. Would would you say we've
    
    gotten lucky in the sense that the leading AI companies, OpenAI, Google Deep Mind, Anthropic, at least there's there's they're they're talking about safety. They're saying they care about safety. Many of these companies were started explicitly because of safety concerns. Is that a good thing or does this mean that perhaps motives change over times? Motives get corrupted, people are influenced by the incentives and and all the rest. The trade-off of inspiring people to create super intelligence in the name of doing it
    
    first so they can do it more safely is that you have people actually creating it faster and more robustly and more competitively with each other, which is bad. But also you have people who at least you know are aware of these concerns and have some amount of value placed in these concerns which is good. You know the first thing I thought when I heard Alman was fired was I don't think I'm going to like the replacement better because you know as much as I have my disagreements of Altman Altman is very clearly very aware of these
    
    questions. If it was like you know I didn't know any other details. So like you know if it ends up being like you know a Greg Brockman style person in charge well that's like that seems worse right we don't know but so my answer is like I'm very happy these people have that background and understand these issues and are in fact you know responsible for this to some degree but yes people's motivations absolutely change and people like lose sight of their original goals and people make trade-offs and they stare into the abyss
    
    and the abyss stars into them and they tell themselves the story. And also you have the problem especially of Alman of you know having this perspective of viewing the people warning about these things as your enemy which they never were but like he he went down a path where he saw it that way and then after the the board issues it became you know that much more so that was the case and that creates an unfortunate situation but yeah no I'd much rather have like Alman or Amdai or Sabis dealing with this
    
    than dealing with people who didn't come from that intellectual tradition and like you in fact see that where like you look at meta you look at deepseec you look at xi like you look at the next tier down and you see people who just do not take these problems remotely seriously at all like I I talked about openi not taking these problems seriously and that's because reality doesn't a curve I they understand the problem exists and they're treating the problem they think that they they think they're taking the problem seriously and
    
    I'm here to inform them that that is insufficient and they are not actually doing so. But good start, right? And then like but like with a meta I'm like no you're actively mocking the problem. like you're actively like throwing fig leaves that are the bare bare minimum at most on top of the situation in a in a certain pessimistic mood or from a certain perspective. This situation seems quite hopeless then because what what can you do if you if you enter the if you enter the game and if you try to
    
    to build an AI company that's responsible maybe you accidentally push in the wrong direction. Is it possible to pause? What do you think of the idea of of pausing AI development or coordinating around a pause? In theory, it's a great idea, at least at some point in the future. It's probably too early on several levels. Like, it's definitely too early in terms of the political will, right? Like, we're not going to get a pause until the AIS are much scarier than they currently are. That's just a reality. And so, I think
    
    pushing for a pause before that happens is not harmful for the most part. I think people who believe in it should do it anyway because like they have different beliefs than I do. But like I think the pause letter was a damaging thing for the chances of humanity's survival because instead of pushing the Overton window, it then created this point of mockery, this point of attack that was very harmful when, you know, six months later the world didn't end or whatever, which is obviously a stupid stupid way of looking at things, but
    
    like we have to face the political realities that we face. Whereas the the case letter right that like just acknowledging the extent of risk of the situation I think was very very helpful in the same way. So the question of pause is we we should be asking how do we pause? We should be looking for a way to pause such that if the major world leaders and governments realize that a pause was necessary, we could then have a pause. That's very different from actually trying to pause right now. And then I think the pause can only happen
    
    when something pretty scary happens that goes wrong or at least some very very scary capabilities are demonstrated in the future. and building the capacity to pause. Is that just does that just consist of building ordinary international cooperation and making sure there's there's communication across countries and what does that consist of? Is it is it more on the technical side? I think it's lar there's a lot of on the politics side obviously building trust opening communication channels you know creating you know
    
    various organizations and structures and drafting language and but also figuring out logistics like figuring out how you would do it like what is this you know we say what's what is a pause what does it mean for a pause doesn't mean shutting down every CPU or or even GPU it doesn't mean that we don't use AI it just means that like we don't do this specific thing what is that specific thing how does that work, you know, how would we enforce it? How would we monitor it? Like how long would it last?
    
    Like what is the, you know, what are the various different conditions and rules and so on? Like if you have to start this this process from scratch after you realize that you need to pause, it's going to be probably too late. You have to work on that now. And there's also the question of like what can we do physically to make this easier, right? If we if we concentrate, you know, compute, we monitor compute in ways that make it much easier to figure out where the large amounts of compute are and
    
    monitor it, shut it down. if we start putting like physical monitoring devices into new hardware, you know, what are the questions that we can do? And I think we should be asking these questions, right? Like we should be asking like if we need to shut things down, how do we do that? If if that's a coordinated pause, voluntary pause, if that's something else, like we need these options and then, you know, we hope to not need them. You know, like the best case scenario from my perspective, obviously the best case
    
    scenario is that everything is fine. There was nothing to worry about. We just create super intelligence that transform the world and it's paradise. But you know barring that obviously like the best case scenario in important senses is that super intelligence is just very difficult and we have a lot of time to figure out how to handle that problem and in the meantime we get all the benefits of the AI we have now which I think will be pretty wonderful. Does that make it more difficult to do the kinds of communication we're engaged in
    
    right now or to talk about the risks just because the AI products we have are so great and they're so useful to people and people like them so much? Certainly the reason why there's a lot of opposition to these things is because AI has so much promise and so much upside and a lot of that is very real and a lot of that is still locked in future advancements although a remarkably large amount of it isn't and it's just a matter of exploiting what we already have but I think genuinely a lot of the people who are basically you know
    
    accelerationists who are just like we need to go forward as fast as possible we have to disregard all these safety concerns it comes from a place of there's so much promise here and also our society has become so opposed to progress and abundance and doing good things in other realms and they see this as the last stand. This seems the only place left to make a big difference. And like, you know, I'd love to be with them fighting for, you know, nuclear power and building housing where people want
    
    to live and doing all the, you know, the most of the rest of what their agenda would be in ordinary politics and to have that be the thing that they focus on and to make the world a much better place that way while we figure this problem out. But they've despared at those problems such that they can't even believe the idea that like someone like me would want to support them in all these other places because it doesn't make sense to them. And does even come to the point where you know because of
    
    this motivation that they need to push forward here at any cost they then you know develop all these preconceptions and you know essentially this misinformation spreads about like what must be the motivations and what must be the thinking of the people who are opposed. It's just madness. But what can you do? What do you say to make the case that AI is different from the other from the other technologies you mentioned there? So why why is AI different from from building new more nuclear power or
    
    building more housing? And why isn't it just another instance of we want to avoid regulating a new technology to death? There's there always many scary predictions and they don't come true. And so why why why is AI different? Because AI is about creating new optimiz new more powerful optimizers, new more intelligent entities that are not human that will in fact like you know before we were building tools, tools that humans used to make things that humans wanted to change the world in ways that humans wanted. And that wasn't a reason
    
    for anarchism. That wasn't a reason to just prefer allow anyone to do anything they ever wanted. But we went way too but we went way too far in trying to constrain what people could do with these tools. But AI is not going to remain a mere tool. AI is going to have intelligence that already in many ways rivals us and will soon surpass us probably. It will have be more competitive in an economic capitalistic context. it will, you know, be a more powerful optimization agent, you know, capable of rearranging the atoms in ways
    
    that, you know, satisfy its preferences and, you know, aren't necessarily attuned to what we want. And, you know, in general, you know, we have every reason to believe that, you know, left to its own devices, that like the things that will survive are the things that are optimized to survive in that future world. And that's not going to be us. It's going to be AIS. And of course, you know, you can go into details to make this much more robust and explain exactly like how in what ways things go wrong or what paths things take or you
    
    know, but like my basic perspective on this point is like, you know, yes, the alignment problem is impossibly difficult, the governance problem is impossibly difficult, etc., etc., but screw all of that. Stop pretending that creating things that are not human, that are smarter than human, more capable than human, more competitive than human, more powerful optimizers than human, that can be copied, be run in parallel, you know, that have unlimited memories, etc., etc., that that is a safe thing to
    
    do. That is a thing that's likely to turn out well for the humans like by default without any interventions. You should just let nature take its course. This is absurd. This is just patently absurd on its face. And we can argue about like what the solution to that is, but the idea that like anarchism is the way is really bizarre that we're like entertaining this suggestion. If we get to 2050 and we have a flourishing human civilization and super intelligence at the same time, why were you wrong? So, I
    
    was probably wrong because first of all, it probably took a lot probably it took a while to get to super intelligence. It probably came at the lower end of that range. It was wrong because it turned out that like the technical solutions were easier than I thought they were and we found them in time. We found ways to leverage the AIS and leverage like our knowledge and so on and figure them out and also we managed to coordinate and steer the outcomes in some fashion in ways that like I am skeptical we can do
    
    but that clearly aren't impossible. And we kind of won the parlay, right? we get all these different things to happen and like therefore things worked out reasonably well. It also like would involve facts about like what leads to human flourishing and other detailed dynamics that like you know working out well but like basically like a lot of things have to go right and nothing go that wrong along the way but yeah like it's certainly like you know 70 is not 99 I have no yeah I'm not I'm not claiming I I don't get if you tell me
    
    it's 2030 and that's happened I'm like a lot more surprised. I have noticed people beginning to use terms like AGI and super intelligence in ways that I don't that don't really jive with my understanding of those terms. For example, Tyler K recently said that that 03 is is AGI in his estimation by by his definition of that. I've also heard you know I've heard discussions of what how how do we do business in the best possible way in a world where we have AGI for example or Sam Alman has talked has talked about AGI coming and and
    
    going and the world not us we will we will kind of get used to it as a society do these terms mean less and less over time I think there's reasonable arguments the goalpost have moved in both directions in different ways that like if it was 2008 eight say just like well before and you suddenly had 03 and you showed it to people they'd go oh wow that's AGI and I think that would be an unreasonable thing for them to say look 03 you know responds to the vast majority of queries much better than the
    
    vast majority of humans it's not crazy to call that AGI I just that's not what we mean by AGI in our current conversation that's not going to have the consequences that you know the thing that we're imagining as AGI would have So I think it's wrong to call it AGI and the majority of people agree with me but I don't think that we should mock Tyler for that statement. I think somewhat for his reasoning but like statement is the statement is not crazy. As a final topic here I would like to discuss trading because this is
    
    something you've done professionally and in some sense you've you've done it in your Magic the Gathering career also trading these cards. How is trading affected by by AI? Do you think it's it's getting more and more difficult to discover good opportunities? That was already true. So, as a general rule, like trading has gotten more difficult across the board for a long time. So, like if you took a modern trader and you sent them back to the 70s or the 80s or whatever, they'd be like, "Free money everywhere. Woo!" Right?
    
    Because like, you know, at some point you're just like, "Oh, I have this Black Souls formula." And you don't. This is going to be fun. But like I you know after a while like it gets harder and harder because like you're competing against other people who are also advancing the technologies and like it's not just response times get faster. It's that like the intelligence behind what everyone's doing gets better and you know you discover various patterns and correlations and systems and then like
    
    they get competed out and they stop working and then like there's all these different complexities that everyone's dealing with and you see this in other markets too like with when I was doing sports betting you know I I found lots of like you know we called it free money basically like various different opportunities to just like take money out of the system fairly easily and like with large edges. And I learned later that like if I had gotten in a, you know, a year or two earlier, they would have been much
    
    bigger than the ones I found. And then several years later, they were much smaller than they they started out, but they were still there. And then like by the time I was done with that industry, like it was a lot harder. There were still ways, but it was a lot harder. And you know the same thing is is true in crypto where like you know when I was trading crypto in 2018 it was very easy as a trader with ordinary skill in the art to make like good returns. And then like I'm not saying it's hard necessarily now but it's harder for
    
    sure. Stock markets the same way. And like that was before AI and AI is only going to accelerate all of this right because AI is now like in the hands of everybody at all sides. But it also can create opportunity because the market has not become situationally aware. So like I was listening to a podcast odd lots recently. It's one of the best podcasts and spread economics and you're listening to a trader talking about China's situation and like the general trade war. And he's like well I saw Deep
    
    Sea came out and then I concluded oh AI can be copied therefore it's worthless. So I shorted the NASDAQ. And like he's like that was a good trade. And I'm like, it was a good trade because it turns out the market is that insufficient, that that unaware of what's going on and has that level of understanding such that people are freaked out about things that were good for the companies involved in many cases like just complete misunderstanding, complete order of magnitude misunderstanding of what happened and
    
    then complete misunderstanding of what it implied. The extent it implied anything, the whole thing should have been mostly priced in already. It was kind of insane. And so, you know, they're also not using AI properly. If they're not even aware of AI, if they can't even think well about AI and its implications, they're clearly not going to use the tools the way you're using the tools. So, you could have an advantage there. Yeah, makes a lot of sense. But in the long term, you know, when the AIs themselves are doing a lot
    
    of the trading and and trading get much much more sophisticated and it's going to be very very difficult to accomplish much and like reaction times are going to go like way way faster. So, Si, thanks for chatting with me. It's been a real pleasure. Yeah, thanks for having me.
    