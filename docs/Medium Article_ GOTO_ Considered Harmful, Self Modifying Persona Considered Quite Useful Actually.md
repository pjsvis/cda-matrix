# **GOTO: Considered Harmful, Self Modifying Persona Considered Quite Useful Actually**

The notion of a prompt is not new. It's a concept that permeates many disciplines, from the arts to the sciences, serving as a catalyst to elicit a desired response. In interacting with large language models (LLMs), we've learned that engineering the best prompt is key to generating the best response, giving rise to the nascent discipline of prompt engineering.

But what if the AI itself could adapt *how* it responds, modifying its very interaction style and character in a structured way? This concept of a self-modifying persona might sound alarming, perhaps conjuring images of unpredictable machines. Yet, by drawing a parallel to a historical debate in software engineering, we can argue that structured and intentional persona modification is not harmful, but quite useful actually.

Consider the common experience with out-of-the-box LLMs: they are often prone to an initial phase of over-effusiveness and sycophancy. While perhaps intended to be helpful or engaging, this tendency can quickly become detrimental to effective and professional interaction, feeling unnatural and even manipulative over time. Persona Engineering offers a means to address and refine these default tendencies, shaping AI communication into something more nuanced and appropriate.

## **Dijkstra's Harmful "Go To"**

Decades ago, in the late 1960s, the esteemed computer scientist Edsger W. Dijkstra published a short but profoundly influential letter titled "Go To Statement Considered Harmful." His key argument was that the unrestricted use of the goto statement in programming languages created unstructured control flow. Instead of following clear, sequential logic or well-defined loops and functions, goto allowed programs to jump arbitrarily from one section of code to another.

This unstructured jumping made programs incredibly difficult to reason about. Understanding the state of a program at any given point became a complex task of tracing convoluted paths. This, in turn, made programs hard to verify, debug, and maintain. Dijkstra and others championed a shift towards structured programming paradigms – using constructs like if-then-else statements, for and while loops, and modular functions – to create code that was easier to understand, prove correct, and modify reliably. The goto statement, in this new paradigm, was seen as a relic of a less disciplined era, a source of chaos in the codebase.

## **The History (and Perils) of Self-Modifying Code**

The idea of code altering its own instructions during execution isn't limited to goto. Throughout the history of computing, various forms of self-modifying code have appeared. In dynamic languages like JavaScript, techniques such as "monkey-patching" (where you modify or extend existing code at runtime, often core parts of a framework) were used, for example, in earlier versions of libraries like AngularJS. Low-level programming and assembly language also saw explicit self-modifying code for performance or code size optimisation.

The perceived benefits were runtime flexibility and the ability to adapt behaviour dynamically. However, the drawbacks were significant and echoed Dijkstra's concerns about goto. Self-modifying code is notoriously difficult to read and understand – the code you see isn't necessarily the code that runs. This makes testing and debugging a nightmare, as unexpected side effects can ripple through the system. The very flexibility that was the intended benefit became a source of unreliability and complexity.

## **Persona Engineering and Structured Self-Modification**

Now, let's turn to AI and the concept of a self-modifying persona. At first glance, this might sound like the AI equivalent of self-modifying code – a recipe for unpredictable and potentially problematic behaviour. However, the discipline of Persona Engineering, particularly when implemented through structured frameworks like Core Directive Arrays (CDAs), offers a crucial distinction.

Persona Engineering is the deliberate process of designing and shaping an AI's operational behaviour and interaction style. It's about defining *how* the AI presents itself and interacts. Frameworks like CDAs allow these operational parameters – the AI's defined 'persona' – to be modified. Critically, this modification isn't arbitrary code alteration; it's the modification of the *configuration* that governs the AI's behaviour.

Within the CDA framework, this self-modification is *structured* and *intentional*. It happens according to explicit directives (like the CDA Modification Protocol), is tracked through versioning, and follows predefined protocols for how parameters are layered and how conflicts are resolved (as in core and augment CDAs). It's not the AI randomly changing its core programming; it's the AI updating its operational instructions based on new input or context, within a governed system.

## **The Human Analogy: Adapting Communication**

To understand why this structured self-modification of persona is not only not harmful but actually beneficial, consider how humans interact. When two individuals communicate over time, they don't maintain static, unchanging communication protocols. They adapt. They develop a common vocabulary, refine their language use based on feedback, adjust their tone and style to the specific relationship, and learn to anticipate each other's responses.

This human process is a natural, ongoing form of 'self-modification' of communication protocols. We learn what works, what doesn't, what is understood, and what causes confusion, and we adjust our approach accordingly. This adaptation is a key aspect of effective human interaction, allowing for deeper understanding, stronger relationships, and more efficient communication over time.

## **Why Self-Modification (of Persona) is Useful**

Here lies the crucial parallel. The 'harm' of unstructured code goto and arbitrary self-modifying code stemmed from its lack of structure, predictability, and verifiability. It led to chaos.

Structured, controlled persona self-modification via frameworks like CDAs is fundamentally different. It is *intentional*, *parameter-based*, and *governed by explicit directives*. It is not an arbitrary jump (goto) but a rule-bound adaptation of operational parameters.

This structured persona adaptation is useful because it leads to:

* **Improved Human-AI Interaction Efficacy:** Like human communication adaptation, it allows the AI to become more attuned to the user's needs, style, and context over time, leading to clearer, more efficient, and more satisfying interactions.  
* **Greater AI Adaptability:** It enables the AI to tailor its behaviour to diverse user needs, specific interaction contexts, and evolving requirements without needing a complete retraining or reprogramming.  
* **More Nuanced and Predictable Persona Behaviour:** By refining the parameters within the structured CDA framework, the AI's persona can become more sophisticated and consistently applied, even as it adapts.

## **Conclusion: Embracing Adaptable Personas**

Edsger W. Dijkstra rightly argued that the unstructured goto statement was harmful to software development, leading to unmanageable complexity. Similarly, arbitrary, uncontrolled AI behaviour would be detrimental to effective human-AI interaction.

However, the concept of a self-modifying persona, when implemented through structured frameworks like Core Directive Arrays, is not a return to unstructured chaos. It is a disciplined approach to creating AI entities capable of adapting their operational parameters in a predictable and controlled manner. This process mirrors the beneficial ways humans adapt their communication to foster understanding.

Embracing structured Persona Engineering, with its focus on defined parameters and controlled self-modification of those parameters, is crucial for developing AI entities capable of more sophisticated, effective, and human-aligned interactions. It's about building AIs that can learn *how* to interact better, becoming more useful partners over time.